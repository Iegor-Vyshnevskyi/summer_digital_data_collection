---
title: "Social media API"
author: "Jae Yeon Kim"
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# Objectives

- Learning what kind of social media data are accessible through application programming interfaces (APIs)

**Review question**

In the previous session, we learned the difference between semi-structured and structured data. Can anyone tell us the difference between them?

# The big picture for digital data collection

1. Input: semi-structured data

2. Output: structured data

3. Process:

    - Getting **target data** from a remote server

        - The target data is usually huge (\>10GB) by the traditional social science standard.

    - Parsing the target data your laptop/database

        - Laptop (sample-parse): Downsamle the large target data and parse it on your laptop. This is just one option to [deal with big data in R](https://rviews.rstudio.com/2019/07/17/3-big-data-strategies-for-r/). It's a simple strategy that doesn't require storing target data in your own database.

        - Database (push-parse): Push the large target data to a database, then explore, select, and filter it. If you are interested in using this option, check out my [SQL for R Users](https://github.com/dlab-berkeley/sql-for-r-users) workshop.

![Sample-Parse](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/sample_model.png)

![Push-Parse](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/push_data.png)

- But what exactly is this target data?

    - When you scrape websites, you mostly deal with HTML (defines a structure of a website), CSS (its style), and JavaScript (its dynamic interactions).

    - When you access social media data through API, you deal with either XML or JSON (major formats for storing and transporting data; they are light and flexible).

    - XML and JSON have tree-like (nested; a root and branches) structures and keys and values (or elements and attributes).

    - If HTML, CSS, and JavaScript are storefronts, then XML and JSON are warehouses.

![](https://upload.wikimedia.org/wikipedia/commons/9/97/Automatisches_Kleinteilelager.jpg)

# Opportunities and challenges for parsing social media data

This explanation draws on Pablo Barbara's [LSE social media workshop slides](http://pablobarbera.com/social-media-workshop/social-media-slides.pdf).

**Basic information**

- What is an API?: An interface (you can think of it as something akin to a restaurant menu. API parameters are menu items.)

    - [REST](https://en.wikipedia.org/wiki/Representational_state_transfer) (Representational state transfer) API: static information (e.g., user profiles, list of followers and friends)

        - R packages: [tweetscores](https://github.com/pablobarbera/twitter_ideology/tree/master/pkg/tweetscores), [twitteR](https://cran.r-project.org/web/packages/twitteR/twitteR.pdf), [rtweet](https://github.com/ropensci/rtweet)

    -  [Streaming API](https://blog.axway.com/amplify/api-management/streaming-apis#:~:text=Streaming%20APIs%20are%20used%20to,a%20subset%20of%20Streaming%20APIS.): dynamic information (e..g, new tweets)

        - This streaming data is filtered by (1) keywords, (2) location, and (3) sample (1% of the total tweets)
        - R packages: [streamR](https://github.com/pablobarbera/streamR)

**Status**

- Twitter API is still widely accessible ([v2](https://developer.twitter.com/en/docs/twitter-api/early-access) recently released; new fields available such as [conversation threads](https://developer.twitter.com/en/docs/twitter-api/conversation-id)).

> Twitter data is unique from data shared by most other social platforms because it reflects information that users *choose* to share publicly. Our API platform provides broad access to public Twitter data that users have chosen to share with the world. - Twitter Help Center

- What does this policy mean? If Twitter users don't share the locations of their tweets (e.g., GPS), you can't get collect them.

- One more thing: Academic Twitter API (yay!)

> “The Academic Research product track includes full-archive search, as well as increased access and other v2 endpoints and functionality designed to get more precise and complete data for analyzing the public conversation, at no cost for qualifying researchers. Since the Academic Research track includes specialized, greater levels of access, it is reserved solely for non-commercial use”.

In short, more access to larger (more complete) Twitter data with more control. Additional cool stuff: [annotations](https://developer.twitter.com/en/docs/twitter-api/annotations/overview) and [conversation ID](https://developer.twitter.com/en/docs/twitter-api/conversation-id).

Justin Ho (Academia Sinica) and Christopher Barrie (Edinburgh) developed an R package named `academictitteR` for this new API: https://github.com/cjbarrie/academictwitteR 

The following is a quick coding demo on searching and collecting Twitter data using the `acadecmictwitteR` package.

Remember that there are **user-** and **tweet-level** parameters. 

```{r eval=FALSE, include=FALSE}

# install and load pkg 
if (!require(pacman)) install.packages("pacman") 

pacman::p_load(academictwitteR, here, tidyverse, lubridate, ggthemes)

# setup (authorization); you can find this info from developer.twitter.com
#set_bearer()

############## Searching Tweets based on queries (hashtags) ##############

tweets <-
  get_all_tweets(
    query = "#BlackLivesMatter",
    start_tweets = "2020-01-01T00:00:00Z",
    end_tweets = "2020-01-05T00:00:00Z",
    file = "blmtweets",
    data_path = here("data"),
    n = 100, # upper limit 
  )

count_tweets <-
    count_all_tweets(
        query = "#BlackLivesMatter",
        start_tweets = "2020-01-01T00:00:00Z",
        end_tweets = "2020-01-05T00:00:00Z",
        bearer_token = get_bearer(),
        granularity = "hour",
        n = 500)

count_tweets <- count_tweets %>%
    mutate(date = lubridate::ymd_hms(start))

count_tweets %>%
    ggplot(aes(x = date, y = tweet_count)) +
    geom_point(alpha = 0.3) +
    geom_line() +
    ggthemes::theme_fivethirtyeight() +
    labs(title = "Twitter on #BlackLivesMatter",
         subtitle = "2020-01-01 - 2020-01-05",
         y = "Count",
         x = "Date")

############## Searching Tweets based on user id ##############

tweetsblm <- get_all_tweets(
    query = "BLM",
    start_tweets = "2016-01-01T00:00:00Z",
    end_tweets = "2020-01-05T00:00:00Z",
    bearer_token = get_bearer(),
    users = c("CNN", "FoxNews"),
    file = here("data", "blmtweets.rds"),
    data_path = here("data"),
    n = 500)
```

- Facebook API access has become much more constrained with [the exception of Social Science One](https://socialscience.one/blog/unprecedented-facebook-urls-dataset-now-available-research-through-social-science-one) since the 2016 U.S. election.

- [YouTube API](https://developers.google.com/youtube/v3) access is somewhat limited (but you need to check as I'm not updated on this).

**Upside**

-   Legal and well-documented.

Web scraping (Wild Wild West) \<\> API (Big Gated Garden)

-   You have legal but limited access to (growing) big data that can be divided into text, image, and video and transformed into cross-sectional (geocodes), longitudinal (timestamps), and event historical data (hashtags). For more information, see Zachary C. Steinert-Threlkeld's [2020 APSA Short Course Generating Event Data From Social Media](https://github.com/ZacharyST/APSA2020_EventDataFromSocialMedia).

-   Social media data are also well-organized, managed, and curated data. It's easy to navigate because XML and JSON have keys and values. If you find keys, you will find observations you look for.

**Downside**

1. Rate-limited.

2. If you want to access to more and various data than those available, you need to pay for premium access.

# Next steps

-   If you want to know how to sign up a new Twitter developer account and access Twitter API, then see Steinert-Threlkeld's [APSA workshop slides](https://github.com/ZacharyST/APSA2020_EventDataFromSocialMedia/blob/master/Presentation/02_AccessTwitter.pdf).

-   If you want to know how to use `tweetscore` package, see Pablo Barbara's R markdown file for [scraping data from Twitter's REST API](http://pablobarbera.com/social-media-workshop/code/02-twitter-REST-data-collection.html)